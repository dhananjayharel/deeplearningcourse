# -*- coding: utf-8 -*-
"""Keras-Basics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NhojC-Z1XFqMkpYpGUIMUO2IIFpTlHp7

# Keras Basics

Welcome to the section on deep learning! We'll be using Keras with a TensorFlow backend to perform our deep learning operations.

This means we should get familiar with some Keras fundamentals and basics!

## Imports
"""

import numpy as np

"""## Dataset

We will use the Bank Authentication Data Set to start off with. This data set consists of various image features derived from images that had 400 x 400 pixels. You should note **the data itself that we will be using ARE NOT ACTUAL IMAGES**, they are **features** of images. In the next lecture we will cover grabbing and working with image data with Keras. This notebook focuses on learning the basics of building a neural network with Keras.

_____
More info on the data set:

https://archive.ics.uci.edu/ml/datasets/banknote+authentication

Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.


Attribute Information:

1. variance of Wavelet Transformed image (continuous) 
2. skewness of Wavelet Transformed image (continuous) 
3. curtosis of Wavelet Transformed image (continuous) 
4. entropy of image (continuous) 
5. class (integer) 

## Reading in the Data Set

We've already downloaded the dataset, its in the DATA folder. So let's open it up.
"""

from numpy import genfromtxt
data = genfromtxt('bank_note_data.txt', delimiter=',')

data

labels = data[:,4]

labels

features = data[:,0:4]

features

X = features
y = labels

"""## Split the Data into Training and Test

Its time to split the data into a train/test set. Keep in mind, sometimes people like to split 3 ways, train/test/validation. We'll keep things simple for now. **Remember to check out the video explanation as to why we split and what all the parameters mean!**
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

X_train

X_test

y_train

y_test

"""## Standardizing the Data

Usually when using Neural Networks, you will get better performance when you standardize the data. Standardization just means normalizing the values to all fit between a certain range, like 0-1, or -1 to 1.

The scikit learn library also provides a nice function for this.

http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html
"""

from sklearn.preprocessing import MinMaxScaler

scaler_object = MinMaxScaler()

scaler_object.fit(X_train)

scaled_X_train = scaler_object.transform(X_train)

scaled_X_test = scaler_object.transform(X_test)

"""Ok, now we have the data scaled!"""

X_train.max()

scaled_X_train.max()

X_train

scaled_X_train

"""## Building the Network with Keras

Let's build a simple neural network!
"""

from keras.models import Sequential
from keras.layers import Dense

# Creates model
model = Sequential()
# 8 Neurons, expects input of 4 features. 
# Play around with the number of neurons!!
model.add(Dense(4, input_dim=4, activation='relu'))
# Add another Densely Connected layer (every neuron connected to every neuron in the next layer)
model.add(Dense(8, activation='relu'))
# Last layer simple sigmoid function to output 0 or 1 (our label)
model.add(Dense(1, activation='sigmoid'))

"""### Compile Model"""

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

"""## Fit (Train) the Model"""

# Play around with number of epochs as well!
model.fit(scaled_X_train,y_train,epochs=50, verbose=2)

"""## Predicting New Unseen Data

Let's see how we did by predicting on **new data**. Remember, our model has **never** seen the test data that we scaled previously! This process is the exact same process you would use on totally brand new data. For example , a brand new bank note that you just analyzed .
"""

scaled_X_test

# Spits out probabilities by default.
# model.predict(scaled_X_test)

model.predict_classes(scaled_X_test)

"""# Evaluating Model Performance

So how well did we do? How do we actually measure "well". Is 95% accuracy good enough? It all depends on the situation. Also we need to take into account things like recall and precision. Make sure to watch the video discussion on classification evaluation before running this code!
"""

model.metrics_names

model.evaluate(x=scaled_X_test,y=y_test)

from sklearn.metrics import confusion_matrix,classification_report

predictions = model.predict_classes(scaled_X_test)

confusion_matrix(y_test,predictions)

print(classification_report(y_test,predictions))

"""## Saving and Loading Models

Now that we have a model trained, let's see how we can save and load it.
"""

model.save('myfirstmodel.h5')

from keras.models import load_model

newmodel = load_model('myfirstmodel.h5')

newmodel.predict_classes(X_test)

"""Great job! You now know how to preprocess data, train a neural network, and evaluate its classification performance!"""